{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "  \n",
    "def load_file(fileName, inputcols,outputcol):\n",
    "    dataset = pd.read_table(\"kseindex.csv\", header=0, sep=\",\", encoding=\"unicode_escape\")\n",
    "    data = dataset[inputcols]\n",
    "    target = dataset[outputcol]\n",
    "\n",
    "    trainingX,testX,trainingY,testY = train_test_split(data,target,test_size=0.33, random_state=43)\n",
    "\n",
    "    # normalizing data\n",
    "    trainingX = (trainingX - trainingX.min()) / (trainingX.max() - trainingX.min())\n",
    "    trainingY = (trainingY -  trainingY.min()) / (trainingY.max() - trainingY.min())\n",
    "    testX = (testX - testX.min()) / (testX.max() - testX.min())\n",
    "    testY = (testY - testY.min()) / (testY.max() - testY.min())\n",
    "  \n",
    "\n",
    "  \n",
    "    # # apply normalization techniques\n",
    "    # IF WE USE df_max_scaled[column] = df_max_scaled[column] /df_max_scaled[column].abs().max() for normalization error\n",
    "    # reduces to 0.02 from 0.08......\n",
    "    \n",
    "    return trainingX,testX,trainingY,testY\n",
    "\n",
    "#print(trainingX.shape, testX.shape, trainingY.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return(1/(1 + np.exp(-x)))\n",
    "\n",
    "# def sigmoid_deriv(x):\n",
    "#     return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def backward_prop(a1,del_out,outputNeurons,hiddenNeurons,learningRate):\n",
    "    #follows w = output- (learningRate*(Etol_weight) throught dot product (error in layers reduced)\n",
    "    change = learningRate* (del_out).dot(np.resize(a1, (outputNeurons,hiddenNeurons)))\n",
    "    # for the gradient\n",
    "    change = np.transpose(np.resize(change,(outputNeurons,hiddenNeurons)))\n",
    "    return change\n",
    "\n",
    "def learn_model(trainingX, trainingY,inputNeurons, hiddenNeurons, outputNeurons, learningRate, epochs):\n",
    "  \n",
    "    classifier = {}\n",
    "    weight_hid = np.random.rand(inputNeurons, hiddenNeurons)*0.01\n",
    "    weight_out = np.random.rand(hiddenNeurons, outputNeurons)*0.01\n",
    "    \n",
    "    training_X = np.array(trainingX)\n",
    "    # print(training_X) \n",
    "    training_Y = np.array(trainingY)\n",
    "    # print(training_Y)\n",
    "\n",
    "    # for j in range(epoch):\n",
    "    for k in range(epochs):\n",
    "        i = 0 #incremented later\n",
    "        \n",
    "        for x in training_X:\n",
    "            # print(x)\n",
    "            output_Hid_lst = []\n",
    "            output_Out_lst= []\n",
    "            errors_lst = []\n",
    "            error_total = 0\n",
    "        \n",
    "            # Forward propagation\n",
    "            # we are going to assign random weights to the input features.\n",
    "            weights = np.random.randn(inputNeurons, hiddenNeurons) * 0.01\n",
    "            # print(weights)    \n",
    "            bias = np.random.randn((hiddenNeurons)) * 0.01\n",
    "            # print(bias)\n",
    "           \n",
    "            # weight = np.dot(x, weights) + bias\n",
    "            # for the hidden layer Forward propagationp.dot(output_Hidden, weights) + bias\n",
    "            hidden_lay = np.dot(x, weight_hid) + bias\n",
    "            # print(hidden_lay)\n",
    "            a1 = sigmoid(hidden_lay)\n",
    "            # print(a1)\n",
    "            output_Hid_lst.append(a1)\n",
    "            # print(output_Hidden)\n",
    "\n",
    "            # for the output layer Forward propagation\n",
    "            output_lay = np.dot(a1, weight_out)\n",
    "            # print(output_lay)\n",
    "            a2 = sigmoid(output_lay)\n",
    "            # print(a2)\n",
    "            output_Out_lst.append(a2)\n",
    "\n",
    "            # Here we have the backward propagation\n",
    "            # First we find the errors for the output layer\n",
    "            # Etol_out = (target-out)* out(1-out) formulae later we dot with a1          \n",
    "            Etol_out = (training_Y[i]-a2)\n",
    "            out_net = a2*(1-a2)\n",
    "            Etol_weight = out_net*Etol_out\n",
    "            # print(Etol_weight)\n",
    "            errors_lst.append(Etol_weight)\n",
    "            # print(errors_lst)\n",
    "        \n",
    "            #for output layer (hidden to output weig change)\n",
    "            Weights_o = backward_prop(a1,Etol_weight,outputNeurons,hiddenNeurons,learningRate)\n",
    "            # print(Weights_o)\n",
    "            weight_out = weight_out + Weights_o \n",
    "            # print(weight_out) \n",
    "            \n",
    "\n",
    "            # Second we find the change for the hidden to output layer\n",
    "            Error_hidden = np.dot(weight_out, Etol_weight)\n",
    "            Etol_hidden = a1*(1-a1)*Error_hidden\n",
    "            # print(Etol_hidden)\n",
    "            Weights_h = backward_prop(x,Etol_hidden,hiddenNeurons,3,learningRate)\n",
    "            # print(Weights_h)\n",
    "            weight_hid = weight_hid + Weights_h\n",
    "            # print(weight_hid) \n",
    "            i = i + 1\n",
    "            \n",
    "    classifier[\"hidden_arr\"] = weight_hid \n",
    "    classifier[\"output_arr\"] = weight_out\n",
    "    \n",
    "    # print(classifier)\n",
    "    \n",
    "    return classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(classifier, testdata):\n",
    "    predicted_val=[]\n",
    "    #array for test data\n",
    "    testx = np.array(testdata)\n",
    "    #Your code to make predictions on test data using the learned model will go here\n",
    "    for i in testx:\n",
    "        hiddenNeurons = 4\n",
    "        bias = np.random.randn((hiddenNeurons)) * 0.01\n",
    "        hidden_lay = np.dot(i, classifier['hidden_arr']) + bias\n",
    "        # print(hidden_lay)\n",
    "        a1 = sigmoid(hidden_lay)\n",
    "        # print(a1)\n",
    "        \n",
    "        # for the output layer Forward propagation\n",
    "        output_lay = np.dot(a1, classifier['output_arr'])\n",
    "        # print(output_lay)\n",
    "        a2 = sigmoid(output_lay)\n",
    "        # print(a2)\n",
    "        predicted_val.append(a2[0])\n",
    "        # print(predicted_val)\n",
    "    return predicted_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeError(actual_class, predicted_class):\n",
    "        \n",
    "    MSE = -1    \n",
    "    #Your code to compute MSE will go here\n",
    "    # for loss we will be using mean square error(MSE)\n",
    "\n",
    "    s =(np.square(actual_class-predicted_class))\n",
    "    MSE = np.sum(s)/len(predicted_class)\n",
    "   \n",
    "    \n",
    "    \n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data.....\n",
      "Learning model.....\n",
      "Classifying test data......\n",
      "Evaluating results.....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0867438978447767"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, target = [\"X1\",\"X2\",\"X3\"],\"Y\"\n",
    "\n",
    "print(\"Loading data.....\")\n",
    "trainingX,testX,trainingY,testY = load_file(\"kseindex.csv\", features, target)\n",
    "\n",
    "print(\"Learning model.....\")\n",
    "model = learn_model(trainingX,trainingY,3,4,1,0.8,10)\n",
    "\n",
    "print(\"Classifying test data......\")      \n",
    "predictedY = predict(model, testX)\n",
    "\n",
    "print(\"Evaluating results.....\")\n",
    "computeError(testY,predictedY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "1e3d2864960cda1cc89e7405ec595e77e7ac30692c1b4230c1dcf8d9a5036813"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
